# -*- coding: utf-8 -*-
"""6_19101532_MdShamimMia.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F67kTGX__EaNT71xOCsxHt8-GDp7HSMx

Importing google drive here
"""

from google.colab import drive
drive.mount('/content/drive')

"""Changing directory"""

import os
directory = "/content/drive/MyDrive/Spring22/CSE422 Lab"
os.chdir(directory)

"""Making DataFrame"""

import pandas as pd
wine_dataframe = pd.read_csv('wine.csv')
wine_dataframe

"""Checking number of rows and columns by calling shape"""

wine_dataframe.shape

wine_dataframe.shape[0]

wine_dataframe.shape[1]

"""Showing first 2 rowns"""

wine_dataframe.head(2)

"""Showing a random row from the dataset"""

wine_dataframe.sample()

"""Showing all the column names"""

wine_dataframe.columns

"""Checking null values"""

wine_dataframe.isnull()

"""Checking how many null values are there for each column and found that there is no missing values"""

wine_dataframe.isnull().sum()

"""Checking overall information of the dataset"""

wine_dataframe.info()

"""Only categorical feature is 'quality' and so encoding it"""

wine_dataframe['quality'].unique()

wine_dataframe['quality'] = wine_dataframe['quality'].map({'good':1, 'bad':0})
print(wine_dataframe[['quality']].head())

"""Scaling the values if they are not in between 0-1"""

need_to_scale = []
for i in wine_dataframe:
  maximum = max(wine_dataframe[i])
  minimum = min(wine_dataframe[i])
  if minimum<0 or maximum>1:
    need_to_scale.append(i)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
for i in need_to_scale:
  scaler.fit(wine_dataframe[[i]])
  wine_dataframe[i] = scaler.transform(wine_dataframe[[i]])
wine_dataframe

"""Dropping unimportant features or selecting only one of the features that produce the same benifit by observing the correlation heatmap"""

for_cor=wine_dataframe.corr()
import seaborn as sns
sns.heatmap(for_cor, cmap="Blues", fmt='.1f', annot = True)

#Dropping irrelevant features
#drop_columns = ['citric acid', 'chlorides', 'total sulfur dioxide', 'density', 'pH', 'sulphates']
#wine_dataframe = wine_dataframe.drop(drop_columns,axis=1)

"""Splitting the dataset into features and label and also splitting the train, test data"""

from sklearn.model_selection import train_test_split

list_of_features = list(wine_dataframe.columns)[:-1]
print(list_of_features)
x_data =  wine_dataframe[list_of_features]
y_data =  wine_dataframe['quality']

x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size=0.2,random_state=1)

"""SVC"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
m1 = SVC(kernel="linear")

m1.fit(x_train, y_train)

y_pred1 = m1.predict(x_test)

score1 = accuracy_score(y_pred1, y_test)
score1

"""MLP"""

from sklearn.neural_network import MLPClassifier

m2 = MLPClassifier(hidden_layer_sizes=(10), activation="relu", max_iter=10000)
m2.fit(x_train, y_train)

y_pred2 = m2.predict(x_test)

score2 = accuracy_score(y_pred2, y_test)
score2

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier

m3 = RandomForestClassifier(n_estimators=100)
m3.fit(x_train, y_train)

y_pred3 = m3.predict(x_test)

score3 = accuracy_score(y_pred3, y_test)
score3

"""PCA"""

half = (wine_dataframe.columns.shape[0]-1)//2
half

from sklearn.decomposition import PCA 
pca = PCA(n_components=half)

principal_components = pca.fit_transform(x_data)
principal_components

pca.explained_variance_ratio_ # how much information we lost

sum(pca.explained_variance_ratio_)

principal_df = pd.DataFrame(data=principal_components, columns=["Principle Component 1", 'Principle Component 2',"Principle Component 3", 'Principle Component 4',"Principle Component 5"])

main_df=pd.concat([principal_df, wine_dataframe[["quality"]]], axis=1)

main_df.head()

x_data = main_df.drop("quality" , axis=1)
y_data = main_df["quality"]

x_train, x_test, y_train, y_test = train_test_split(x_data, 
                                                    y_data,
                                                    test_size = 0.2,
                                                    random_state=1)

from sklearn.svm import SVC

m1 = SVC(kernel="linear")

m1.fit(x_train, y_train)

y_pred1 = m1.predict(x_test)

score11 = accuracy_score(y_pred1, y_test)
score11

from sklearn.neural_network import MLPClassifier

m2 = MLPClassifier(hidden_layer_sizes=(10), activation="relu", max_iter=10000)
m2.fit(x_train, y_train)

y_pred2 = m2.predict(x_test)

score22 = accuracy_score(y_pred2, y_test)
score22

from sklearn.ensemble import RandomForestClassifier

m3 = RandomForestClassifier(n_estimators=100)
m3.fit(x_train, y_train)

y_pred3 = m3.predict(x_test)

score33 = accuracy_score(y_pred3, y_test)
score33

import numpy as np
import matplotlib.pyplot as plt
 
# set width of bar
barWidth = 0.20
fig = plt.subplots(figsize =(12, 8))
 
# set height of bar
svm = [score1, score11]
mlp = [score2, score22]
rfc = [score3, score33]
 
# Set position of bar on X axis
br1 = np.arange(len(svm))
br2 = [x + barWidth for x in br1]
br3 = [x + barWidth for x in br2]
 
# Make the plot
plt.bar(br1, svm, color ='r', width = barWidth,
        edgecolor ='grey', label ='SVM')
plt.bar(br2, mlp, color ='g', width = barWidth,
        edgecolor ='grey', label ='MLP')
plt.bar(br3, rfc, color ='b', width = barWidth,
        edgecolor ='grey', label ='RFC')
 
# Adding Xticks
plt.ylabel('Score', fontweight ='bold', fontsize = 15)
plt.xticks([r + barWidth for r in range(len(svm))],
        ['Before-PCA', 'After-PCA'])
 
plt.legend()
plt.show()